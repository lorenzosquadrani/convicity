{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "try_convicity.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOjy28DIe5pX/jyAmVuGP/d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenzosquadrani/convicity/blob/main/try_convicity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAJA3GzqR4zT"
      },
      "source": [
        "#Libraries\n",
        "We load convicity and plasticity libraries, as well as Pytorch libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZyDM04jP5ww",
        "outputId": "91364e78-7a54-41f4-fdc0-8647aa94a79b"
      },
      "source": [
        "#clone convicity and plasticity repository in order to import the libraries\n",
        "!git clone https://github.com/lorenzosquadrani/convicity.git\n",
        "!git clone https://github.com/lorenzosquadrani/plasticity.git\n",
        "!mv /content/plasticity/plasticity/__version__.py.in /content/plasticity/plasticity/__version__.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'convicity'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 83 (delta 29), reused 55 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (83/83), done.\n",
            "Cloning into 'plasticity'...\n",
            "remote: Enumerating objects: 1430, done.\u001b[K\n",
            "remote: Counting objects: 100% (573/573), done.\u001b[K\n",
            "remote: Compressing objects: 100% (369/369), done.\u001b[K\n",
            "remote: Total 1430 (delta 366), reused 376 (delta 202), pack-reused 857\u001b[K\n",
            "Receiving objects: 100% (1430/1430), 59.47 MiB | 38.47 MiB/s, done.\n",
            "Resolving deltas: 100% (911/911), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qoqrag9KRoMl",
        "outputId": "754864c0-e74d-40e7-9b97-e9d6cff751cf"
      },
      "source": [
        "cd plasticity/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/plasticity\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNfUIIteRqBx"
      },
      "source": [
        "from convicity.cBCM.cBCM import cBCM\n",
        "\n",
        "from plasticity.model.optimizer import SGD\n",
        "from plasticity.model.weights import Normal\n",
        "from convicity.cBCM.utils import view_weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXxa3UFVR98I"
      },
      "source": [
        "#Load the Mnist Dataset\n",
        "Loading the training dataset, that was contained in convicity repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPeG4OZpz6nh"
      },
      "source": [
        "#load the data\n",
        "trainset = datasets.MNIST('/content/convicity/', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "\n",
        "\n",
        "testset  =  datasets.MNIST('/content/convicity/', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=1000, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaEmJPbQSFly"
      },
      "source": [
        "# Model\n",
        "Define a simple deep network (1 convolutional layer, two linear layers). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbXTtVlYSC52"
      },
      "source": [
        "class LittleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LittleNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.conv_relu_stack = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5, stride=1, padding=0),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(8*24*24, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_relu_stack(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear_relu_stack(x)\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHUP9Rc-ln0e"
      },
      "source": [
        "# Results using cBCM to train the convolutional layer\n",
        "We first train a cBCM model and initialize the parameters of the convolutional layer. Then, by keeping convolutional fixed, we train the rest of the net using SGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IJbNiKVzvfY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d47e1a3-9ccf-4657-dd20-f939b5fad623"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hb_X8M0z1bT"
      },
      "source": [
        "def train(model, device, trainloader, optimizer, loss_function):\n",
        "    model.train()\n",
        "    for data, target in trainloader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_function(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "\n",
        "def test(model, device, testloader, loss_function):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in testloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += loss_function(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    return test_loss, accuracy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVTd7JmW0AeL",
        "outputId": "615ba491-5303-4c3f-f1f7-da9b90710acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7eff7c903450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akMozCxxKVo0"
      },
      "source": [
        "X = np.array(trainset.data, dtype = 'float16')/255."
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZV8kA9CVS6G",
        "outputId": "1af090aa-9fcb-4de0-9e67-f81ac17a9453",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = cBCM(\tn_filters = 8, kernel_size = 5,\n",
        "\t\t\t\tnum_epochs=5, batch_size = 100, activation = 'relu',\n",
        "\t\t\t\toptimizer = SGD(lr=4e-2), weights_init = Normal(), interaction_strength = -0.,\n",
        "\t\t\t\trandom_state = 42, verbose = True)\n",
        "\n",
        "model.fit(X[:1000])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 222/5760 [00:00<00:02, 2210.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5760/5760 [00:02<00:00, 2061.64it/s]\n",
            "  3%|▎         | 199/5760 [00:00<00:02, 1984.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5760/5760 [00:02<00:00, 2026.06it/s]\n",
            "  4%|▎         | 205/5760 [00:00<00:02, 2047.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5760/5760 [00:02<00:00, 1997.99it/s]\n",
            "  4%|▎         | 202/5760 [00:00<00:02, 2014.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5760/5760 [00:02<00:00, 2009.93it/s]\n",
            "  4%|▎         | 215/5760 [00:00<00:02, 2145.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5760/5760 [00:02<00:00, 2030.00it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY4dorF5XpnK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "058b5861-68f5-4d0f-90d8-721688c83154"
      },
      "source": [
        "view_weights(model, rows = 2, cols = 4)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if-nhSExZ6NU"
      },
      "source": [
        "lr = 1e-3\n",
        "num_epochs = 100"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JyAsWgu2IY7"
      },
      "source": [
        "net = LittleNet().to(device)\n",
        "optimizer = optim.SGD(net.parameters(), lr= lr)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, (40,30), gamma=0.1)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QhSHHt-bH19"
      },
      "source": [
        "with torch.no_grad():\n",
        "    net.conv_relu_stack[0].weight.data = torch.tensor(model.weights.reshape(8,1,5,5), device = 'cuda', dtype=torch.float32)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLCte4GfcKwg"
      },
      "source": [
        "net.conv_relu_stack[0].weight.requires_grad = False"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYzI7DbjaQxl",
        "outputId": "f14d89b8-c014-4185-c3e0-11ec29cc3dc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch %d/ %d\" %(epoch,num_epochs ))\n",
        "    train(net, device, train_loader, optimizer, loss_function)\n",
        "    scheduler.step()\n",
        "    x, y = test(net, device, test_loader, loss_function)\n",
        "    print(\"Test Loss and Accuracy: %f, %d %%\" %(x,y))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/ 100\n",
            "Test Loss and Accuracy: 0.000421, 86 %\n",
            "Epoch 1/ 100\n",
            "Test Loss and Accuracy: 0.000257, 92 %\n",
            "Epoch 2/ 100\n",
            "Test Loss and Accuracy: 0.000207, 93 %\n",
            "Epoch 3/ 100\n",
            "Test Loss and Accuracy: 0.000179, 94 %\n",
            "Epoch 4/ 100\n",
            "Test Loss and Accuracy: 0.000169, 94 %\n",
            "Epoch 5/ 100\n",
            "Test Loss and Accuracy: 0.000151, 95 %\n",
            "Epoch 6/ 100\n",
            "Test Loss and Accuracy: 0.000141, 95 %\n",
            "Epoch 7/ 100\n",
            "Test Loss and Accuracy: 0.000139, 95 %\n",
            "Epoch 8/ 100\n",
            "Test Loss and Accuracy: 0.000128, 95 %\n",
            "Epoch 9/ 100\n",
            "Test Loss and Accuracy: 0.000122, 96 %\n",
            "Epoch 10/ 100\n",
            "Test Loss and Accuracy: 0.000118, 96 %\n",
            "Epoch 11/ 100\n",
            "Test Loss and Accuracy: 0.000116, 96 %\n",
            "Epoch 12/ 100\n",
            "Test Loss and Accuracy: 0.000114, 96 %\n",
            "Epoch 13/ 100\n",
            "Test Loss and Accuracy: 0.000110, 96 %\n",
            "Epoch 14/ 100\n",
            "Test Loss and Accuracy: 0.000106, 96 %\n",
            "Epoch 15/ 100\n",
            "Test Loss and Accuracy: 0.000106, 96 %\n",
            "Epoch 16/ 100\n",
            "Test Loss and Accuracy: 0.000102, 96 %\n",
            "Epoch 17/ 100\n",
            "Test Loss and Accuracy: 0.000100, 96 %\n",
            "Epoch 18/ 100\n",
            "Test Loss and Accuracy: 0.000098, 97 %\n",
            "Epoch 19/ 100\n",
            "Test Loss and Accuracy: 0.000098, 96 %\n",
            "Epoch 20/ 100\n",
            "Test Loss and Accuracy: 0.000099, 96 %\n",
            "Epoch 21/ 100\n",
            "Test Loss and Accuracy: 0.000095, 97 %\n",
            "Epoch 22/ 100\n",
            "Test Loss and Accuracy: 0.000093, 96 %\n",
            "Epoch 23/ 100\n",
            "Test Loss and Accuracy: 0.000091, 97 %\n",
            "Epoch 24/ 100\n",
            "Test Loss and Accuracy: 0.000092, 97 %\n",
            "Epoch 25/ 100\n",
            "Test Loss and Accuracy: 0.000090, 97 %\n",
            "Epoch 26/ 100\n",
            "Test Loss and Accuracy: 0.000090, 97 %\n",
            "Epoch 27/ 100\n",
            "Test Loss and Accuracy: 0.000091, 97 %\n",
            "Epoch 28/ 100\n",
            "Test Loss and Accuracy: 0.000088, 97 %\n",
            "Epoch 29/ 100\n",
            "Test Loss and Accuracy: 0.000088, 97 %\n",
            "Epoch 30/ 100\n",
            "Test Loss and Accuracy: 0.000087, 97 %\n",
            "Epoch 31/ 100\n",
            "Test Loss and Accuracy: 0.000087, 97 %\n",
            "Epoch 32/ 100\n",
            "Test Loss and Accuracy: 0.000087, 97 %\n",
            "Epoch 33/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 34/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 35/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 36/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 37/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 38/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 39/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 40/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 41/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 42/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 43/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 44/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 45/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 46/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 47/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 48/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 49/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 50/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 51/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 52/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 53/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 54/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 55/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 56/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 57/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 58/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 59/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 60/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 61/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 62/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 63/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 64/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 65/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 66/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 67/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 68/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 69/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 70/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 71/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 72/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 73/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 74/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 75/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 76/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 77/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 78/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 79/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 80/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 81/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 82/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 83/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 84/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 85/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 86/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 87/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 88/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 89/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 90/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 91/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 92/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 93/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 94/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 95/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 96/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 97/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 98/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n",
            "Epoch 99/ 100\n",
            "Test Loss and Accuracy: 0.000086, 97 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Jm_8jZWNPM"
      },
      "source": [
        "#Training without cBCM\n",
        "Now we train the same model, without using cBCM. The convolutional layer will be trained togheter with others, using SGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA-NQxLlnk7P"
      },
      "source": [
        "lr = 1e-3\n",
        "num_epochs = 100"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsWRnoJ-neqL"
      },
      "source": [
        "model = LittleNet().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr= lr)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, (40,30), gamma=0.1)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eQNislSrT16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f262a4-0c6a-4df1-d7e0-ac27a4b2f6a5"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch %d/ %d\" %(epoch,num_epochs ))\n",
        "    train(model, device, train_loader, optimizer, loss_function)\n",
        "    scheduler.step()\n",
        "    x, y = test(model, device, test_loader, loss_function)\n",
        "    print(\"Test Loss and Accuracy: %f, %d %%\" %(x,y))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/ 100\n",
            "Test Loss and Accuracy: 0.002253, 22 %\n",
            "Epoch 1/ 100\n",
            "Test Loss and Accuracy: 0.002183, 42 %\n",
            "Epoch 2/ 100\n",
            "Test Loss and Accuracy: 0.002104, 58 %\n",
            "Epoch 3/ 100\n",
            "Test Loss and Accuracy: 0.002009, 63 %\n",
            "Epoch 4/ 100\n",
            "Test Loss and Accuracy: 0.001891, 67 %\n",
            "Epoch 5/ 100\n",
            "Test Loss and Accuracy: 0.001747, 70 %\n",
            "Epoch 6/ 100\n",
            "Test Loss and Accuracy: 0.001579, 73 %\n",
            "Epoch 7/ 100\n",
            "Test Loss and Accuracy: 0.001398, 76 %\n",
            "Epoch 8/ 100\n",
            "Test Loss and Accuracy: 0.001218, 79 %\n",
            "Epoch 9/ 100\n",
            "Test Loss and Accuracy: 0.001055, 81 %\n",
            "Epoch 10/ 100\n",
            "Test Loss and Accuracy: 0.000919, 83 %\n",
            "Epoch 11/ 100\n",
            "Test Loss and Accuracy: 0.000810, 84 %\n",
            "Epoch 12/ 100\n",
            "Test Loss and Accuracy: 0.000725, 85 %\n",
            "Epoch 13/ 100\n",
            "Test Loss and Accuracy: 0.000660, 86 %\n",
            "Epoch 14/ 100\n",
            "Test Loss and Accuracy: 0.000608, 86 %\n",
            "Epoch 15/ 100\n",
            "Test Loss and Accuracy: 0.000566, 87 %\n",
            "Epoch 16/ 100\n",
            "Test Loss and Accuracy: 0.000533, 87 %\n",
            "Epoch 17/ 100\n",
            "Test Loss and Accuracy: 0.000505, 88 %\n",
            "Epoch 18/ 100\n",
            "Test Loss and Accuracy: 0.000481, 88 %\n",
            "Epoch 19/ 100\n",
            "Test Loss and Accuracy: 0.000462, 88 %\n",
            "Epoch 20/ 100\n",
            "Test Loss and Accuracy: 0.000445, 88 %\n",
            "Epoch 21/ 100\n",
            "Test Loss and Accuracy: 0.000431, 88 %\n",
            "Epoch 22/ 100\n",
            "Test Loss and Accuracy: 0.000418, 89 %\n",
            "Epoch 23/ 100\n",
            "Test Loss and Accuracy: 0.000407, 89 %\n",
            "Epoch 24/ 100\n",
            "Test Loss and Accuracy: 0.000397, 89 %\n",
            "Epoch 25/ 100\n",
            "Test Loss and Accuracy: 0.000388, 89 %\n",
            "Epoch 26/ 100\n",
            "Test Loss and Accuracy: 0.000380, 89 %\n",
            "Epoch 27/ 100\n",
            "Test Loss and Accuracy: 0.000373, 89 %\n",
            "Epoch 28/ 100\n",
            "Test Loss and Accuracy: 0.000367, 90 %\n",
            "Epoch 29/ 100\n",
            "Test Loss and Accuracy: 0.000361, 90 %\n",
            "Epoch 30/ 100\n",
            "Test Loss and Accuracy: 0.000360, 90 %\n",
            "Epoch 31/ 100\n",
            "Test Loss and Accuracy: 0.000360, 90 %\n",
            "Epoch 32/ 100\n",
            "Test Loss and Accuracy: 0.000359, 90 %\n",
            "Epoch 33/ 100\n",
            "Test Loss and Accuracy: 0.000359, 90 %\n",
            "Epoch 34/ 100\n",
            "Test Loss and Accuracy: 0.000358, 90 %\n",
            "Epoch 35/ 100\n",
            "Test Loss and Accuracy: 0.000357, 90 %\n",
            "Epoch 36/ 100\n",
            "Test Loss and Accuracy: 0.000357, 90 %\n",
            "Epoch 37/ 100\n",
            "Test Loss and Accuracy: 0.000356, 90 %\n",
            "Epoch 38/ 100\n",
            "Test Loss and Accuracy: 0.000356, 90 %\n",
            "Epoch 39/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 40/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 41/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 42/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 43/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 44/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 45/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 46/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 47/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 48/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 49/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 50/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 51/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 52/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 53/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 54/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 55/ 100\n",
            "Test Loss and Accuracy: 0.000355, 90 %\n",
            "Epoch 56/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 57/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 58/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 59/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 60/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 61/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 62/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 63/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 64/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 65/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 66/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 67/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 68/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 69/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 70/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 71/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 72/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 73/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 74/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 75/ 100\n",
            "Test Loss and Accuracy: 0.000354, 90 %\n",
            "Epoch 76/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 77/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 78/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 79/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 80/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 81/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 82/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 83/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 84/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 85/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 86/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 87/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 88/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 89/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 90/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 91/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 92/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 93/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 94/ 100\n",
            "Test Loss and Accuracy: 0.000353, 90 %\n",
            "Epoch 95/ 100\n",
            "Test Loss and Accuracy: 0.000352, 90 %\n",
            "Epoch 96/ 100\n",
            "Test Loss and Accuracy: 0.000352, 90 %\n",
            "Epoch 97/ 100\n",
            "Test Loss and Accuracy: 0.000352, 90 %\n",
            "Epoch 98/ 100\n",
            "Test Loss and Accuracy: 0.000352, 90 %\n",
            "Epoch 99/ 100\n",
            "Test Loss and Accuracy: 0.000352, 90 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}